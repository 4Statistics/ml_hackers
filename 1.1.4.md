## 1.1.4 机器学习的R基础 ##

正如我们在开头说的，我们坚信学习新技能的最好方式就是带着自己想要解决的问题或想要回答的疑问开始。从更高的层次来看自己的工作，会让你的案例学习更加有效率。在这个R基本概念的复习过程中，我们不会从一个具体的机器学习问题入手，但是我们会在用R进行数据工作的时候遇到一些数据处理相关的问题。我们会在后面的案例中看到，为了让数据适应我们的分析格式，我们常常会在输出的格式处理上花费大量的时间，而进行分析编码的时间则比较少。

下面这个例子是一个很有娱乐价值的问题。最近，数据服务商 Infochimps.com 发布了一份包含60,000个UFO目击报告的文档，这份数据来源于世界各地上百年的报告。虽然这是一份国际化的数据集，但是主要的目击数据还是来源于美国。根据这份数据的时间和各种细节记录，我们可能会提出以下的问题：UFO目击是否有季节性的趋势？美国不同地区所发现的UFO目击现象有何不同？

这是一个很不错的用来进行数据探索的数据集，因为它内容很丰富，结构清晰，并且很有趣。同时由于他是由大文本构成的，这将对本书后面的案例分析很有帮助，因为我们的案例分析的数据都是大文本。对于这样的文本文件，常常会有不少麻烦的部分要处理，我们会使用基本的R函数和一些扩展库来清理和组织数据。本节我们会带着你一步一步，尝试一个完整的分析用来回答我们上面提到的几个问题。你可以从本章文件夹下的 ufo_sightings.R 中获得源码。我们会从数据加载和扩展包开始分析。

### 加载函数库及数据 ###
首先, 我们会加载 ggplot2 包, 我们将会在最后进行可视化分析的时候使用它：

    library(ggplot2)

当加载 ggplot2 的时候，你会注意到这个包同时加载了另外两个依赖包：plyr 和 reshape包。这两个包都是用来操作和组织数据的，在本例中我们将会使用plyr来收集和组织数据。

下一步是从ufo_awesome.tsv文件(在第一章的data/ufo/目录下，可以在README中找到链接)中把数据加载到R环境。需要注意的是，这个文件使用了制表符来区分数据，也就是说我们需要用 read.delim 函数来读取数据。由于R严格遵自己的守默认规则，我们要格外小心谨慎的注意自己所使用函数的默认参数。假如我们没用用过 read.delim 函数，需要看一下帮助手册，或者说我们完全不知道有这么个函数来处理制表符，我们要怎么做呢？R提供了一组有用的函数来帮助我们：

    ?read.delim                 # 打开一个函数的帮助文件
    ??base::delim               # 在base包中的所有帮助文件中搜索delim
    help.search("delimited")    # 在所有帮助文件中搜索delimited
    RSiteSearch("parsing text") # 在R官方站点中搜索parsing text

第一个例子中，我们在函数的开头添加了一个问号，它将会打开指定函数的帮助文件，这是R中的一个很有用的快捷操作。我们同样可以使用??和::组合在包内搜索指定的关键词。双问号表示搜索一个指定单词。在这个例子中我们用双冒号在base包中搜索delim词。R同样支持简短的helo.search和RSiteSearch来进行基本搜索。help.search会搜索所有已安装包的帮助文件，RsiteSearch会搜索R官方站点和邮件列表。由于本书的R指南比较精简，我们强烈建议你自己通过这些搜索功能来浏览具体的R函数。

对于UFO的数据来说，我们需要为read.delim函数手动提供一些参数以便于读取数据。首先，我们需要告诉函数我们的数据是如何分割的。我们知道这是一个用制表符分割的文件，所以我们把delim的seq参数设置为制表符（\t）。然后，当delim函数读取数据时，R会尝试把每一列的数据转换成R自己可用的合适类型。在我们的例子中，所有的列都是字符串，但是read.*的所有函数默认都是把字符串转换为factor类型，factor类型是一种分类变量（categorical variable），这不是我们所期望的。因此我们要把stringsAsFactors设置为FALSE来防止转换。事实上，关闭这项默认特性是很好的习惯，尤其是当你在处理不熟悉的数据时。另外，这组数据的第一行没有表头(列名)，因此我们需要把header属性同样设置为FALSE。最后，数据中有很多空元素，我们希望这些被R标记为NA的值以空字符串的形式出现，我们可以现实的定义na.string为空即可：

    ufo<-read.delim("data/ufo/ufo_awesome.tsv", sep="\t", stringsAsFactors=FALSE,header=FALSE, na.strings="")

>>分类变量（categorical variable）是一种用来表示观测对象在所属分类的类型。在统计学中，分类变量非常重要，这是因为我们可能会对某个观察对象为何属于某种类型感兴趣。在R中，我们使用factor类型来表示分类变量，这在本质上其实是使用数字索引来表示不同的字符串标签。在本例中，我们会把某种字符串--比如州的简写--使用as.factor转换成分类变量，这会给每一个州简写指定一个唯一的数字编号。我们将会在后面多次重复进行这个过程。

我们现在有了一个包含所有UFO数据的数据帧(data frame)！当你和数据帧打交道时，尤其当这些数据来自于外部数据源的时候，手工的进行数据检查始终是个很好习惯。用来进行这项工作的两个非常好的函数是head和tail，这两个函数分别打印数据帧中的最前和最后的六行。

    head(ufo)
    V1            V2            V3         V4        V5              V6
    1 19951009 19951009     Iowa City, IA <NA>      <NA> Man repts. witnessing "flash..
    2 ...
    3 ...

数据帧的第一个明显问题是所有的列名都是自动生成的。使用当前数据集的文档作参考，我们可以给列赋予更加有意义的名字。有意义的列名对于数据帧来说是非常重要的，这会让你的代码和输出更加容易理解。names函数即可以访问列标签的数据结构又可以指定这些列标签的值。我们可以使用这个函数，根据数据集的文档构造一个与列名相关的向量，然后使用names函数重新定义列名:

    names(ufo)<-c("DateOccurred","DateReported","Location","ShortDescription", "Duration","LongDescription")

根据head函数的输出和文档资料，我们知道前两列是日期数据。和在其他语言中一样，R把日期作为一种特殊类型，我们通常希望这些日期字符串可以转换为真正的日期类型。为了这个目的，我们需要使用as.Date函数，它接收一个日期字符串并尝试转换为日期对象。当前的UFO数据中，日期字符串是以YYYYMMDD这种非正常形式来存储的。因此，我们需要给as.Date函数指定一个格式来告诉它怎么转换。我们从DateOccurred列开始：

    ufo$DateOccurred<-as.Date(ufo$DateOccurred, format="%Y%m%d")
    Error in strptime(x, format, tz = "GMT") : input string is too long

我们在这里遇到了第一个错误！虽然有一点模糊，但其中的“input string is too long”表明DateOccurred列的某些数据长度超出了我们设定格式的预期。为什么会出现这样的情况？我们在处理一个很大的文件，非常有可能其中的某些数据在源文件中的格式乱了。假如真是这样的话，那么那些出错的部分就无法正确的被解析，然后就会出现上面的问题。由于我们是和真实的数据打交道，所以很有必要手工清理一下数据。

### 转换日期字符串和处理异常数据 ###
为了解决这个问题，我们首先要找到有错误数据的行，然后再决定怎么处理。我们正好知道我们遇到的错误是”数据太长“,而正确的数据应该总是8个字符，比如”YYYYMMDD“这样的，所以我们只需要找到那些超过8个字符的就行了。实践中，我们通常会先用head函数看一下错误数据具体是什么样的，所以我们会在一段逻辑语句上使用head函数，以找出几个出错的行。

然后，为了删除这些错误行，我们会使用ifelse函数来构造一个TRUE和FALSE向量用来表示数据是不是正确。这个函数是普通的if-esle逻辑的向量版本。在R中，我们会看到很多的向量化的逻辑操作。大多数这些操作比直接遍历一个向量要高效（并非全部）：

    head(ufo[which(nchar(ufo$DateOccurred)!=8 | nchar(ufo$DateReported)!=8),1])
    [1] "ler@gnv.ifas.ufl.edu"
    [2] "0000"
    [3] "Callers report sighting a number of soft white balls of lights headingin an easterly directing then changing direction to the west beforespeeding off to the north west."
    [4] "0000"
    [5] "0000"
    [6] "0000"
    good.rows<-ifelse(nchar(ufo$DateOccurred)!=8 | nchar(ufo$DateReported)!=8,FALSE, TRUE)
    length(which(!good.rows))
    [1] 731 
    ufo<-ufo[good.rows,]

We use several useful R functions to perform this search. We need to know the length of the string in each entry of DateOccurred and DateReported, so we use the nchar function to compute this. If that length is not equal to eight, then we return FALSE. Once we have the vectors of Booleans, we want to see how many entries in the data frame have been malformed. To do this, we use the which command to return a vector of vector indices that are FALSE. Next, we compute the length of that vector to find the number of bad entries. With only 371 rows not conforming, the best option is to simply remove these entries and ignore them. At first, we might worry that losing 371 rows of data is a bad idea, but there are over 60,000 total rows, and so we will simply ignore those malformed rows and continue with the conversion to Date types:

    ufo$DateOccurred<-as.Date(ufo$DateOccurred, format="%Y%m%d") 
    ufo$DateReported<-as.Date(ufo$DateReported, format="%Y%m%d")

下一步，我们需要清理和组织地理位置数据。重新用之前提到的head函数看一下，可以发现地区使用的是“城市，州”这样的格式。我们可以使用R的正则表达式来把这些地区字符串分割成不同几列然后去掉那些不符合要求的数据。这一步很重要，因为我们只对在美国境内发生的UFO目击事件感兴趣。

### 整理地区数据 ###
为了进行这些操作，我们首先需要构造一个用来接收一个字符串参数的函数，这个函数在内部执行处理过程。然后我们会使用另一个向量化的函数对每条数据调用该函数：

    get.location<-function(l) {
        split.location<-tryCatch(strsplit(l,",")[[1]], error= function(e) return(c(NA, NA))) 
        clean.location<-gsub("^ ","",split.location)
        if (length(clean.location)>2) {
            return(c(NA,NA)) 
        }else { 
            return(clean.location)
        } 
    }

这个函数中有一些细节需要注意到。首先，我们把strsplit函数放到了错误处理函数tryCatch中，再次强调，并不是所有的数据都是“城市，州”的格式，事实上有些数据里什么也没有。如果strsplit函数没有收到匹配的字符串，就会抛出一个错误，我们需要捕获这个错误。在本例中，当没有字符串可以切割的时候，我们就返回一个NA来表示当前行的数据不合法。其次，原始的数据包含一些前导空白符，因此我们要用gsub函数（R正则表达式的一部分）来移除这些空白字符。最后，我们增加了一些确认检查，保证只有长度为2的向量才能返回，因为许多非美国地名会生成更长的向量，在本例中我们会让这些数据返回NA向量。

我们会使用lapply函数(list apply的缩写)来遍历地区列的所有字符串，并对每一行调用上面定义的函数。正如前面提到的，apply系列的函数在R中都非常有用。他们都是apply(vector, function)的形式，并且返回向量化的数据。在我们的例子中，我们使用lapply用以返回一个list：

    city.state<-lapply(ufo$Location, get.location) 
    head(city.state)
    [[1]]
    [1] "Iowa City" "IA"
    [[2]]
    [1] "Milwaukee" "WI"
    [[3]]
    [1] "Shelton" "WA"
    [[4]]
    [1] "Columbia" "MO"
    [[5]]
    [1] "Seattle" "WA"
    [[6]]
    [1] "Brunswick County" "ND"

如你所见，R中的list是以键值对的形式存储的，双括号内的是key，单括号内的是value。在我们的例子中key是一个整数，list同样支持string作为key。把数据放在list虽然方便，但并不是我们所需要的，我们希望能在数据帧（data frame）中添加两列表示城市和州。我们需要把这个list转换成两列的矩阵：

    location.matrix<-do.call(rbind, city.state)
    ufo<-transform(ufo, USCity=location.matrix[,1], USState=tolower(location.matrix[,2]),stringsAsFactors=FALSE)

我们使用do.call函数来从list构造一个矩阵。与apply的功能相似，只不过do.call是对list进行迭代。我们会经常使用lapply和do.call的组合来操作数据。在当前的例子中，我们使用rbind(row bind)来把所有city.state中的行数据变成矩阵的一行。为了把结果放到data frame中，我们使用transform函数。

### 处理超出考虑范围的数据 ###
最后要解决的数据清理问题是，有一些符合地点格式要求的数据并非来自美国，比如其中一些就来自加拿大。幸运的是，加拿大的省简称与美国的州简称没有相同的，所以我们可以只取出符合美国州简称的那部分数据集就行了：

    us.states<-c("ak","al","ar","az","ca","co","ct","de","fl","ga","hi","ia","id","il", "in","ks","ky","la","ma","md","me","mi","mn","mo","ms","mt","nc","nd","ne","nh", "nj","nm","nv","ny","oh","ok","or","pa","ri","sc","sd","tn","tx","ut","va","vt", "wa","wi","wv","wy")
    ufo$USState<-us.states[match(ufo$USState,us.states)]
    ufo$USCity[is.na(ufo$USState)]<-NA

To find the entries in the USState column that do not match a US state abbreviation, we use the match function. This function takes two arguments: first, the values to be matched, and second, those to be matched against. The function returns a vector of the same length as the first argument in which the values are the index of entries in that vector that match some value in the second vector. If no match is found, the function returns NA by default. In our case, we are only interested in which entries are NA, as these are the entries that do not match a US state. We then use the is.na function to find which entries are not US states and reset them to NA in the USState column. Finally, we also set those indices in the USCity column to NA for consistency.
Our original data frame now has been manipulated to the point that we can extract from it only the data we are interested in. Specifically, we want a subset that includes only US incidents of UFO sightings. By replacing entries that did not meet this criteria in the previous steps, we can use the subset command to create a new data frame of only US incidents:

    ufo.us<-subset(ufo, !is.na(USState))
    head(ufo.us)
    ...



### 收集整理数据 ###
现在，我们的数据已经准备好可以进行分析了！这些数据主要包含两个纬度的信息，地理位置和时间。前面的几节我们主要精力集中在地理位置，现在我们要对时间进行研究了。首先，我们使用summary函数来了解这组数据的时间周期：

    summary(ufo.us$DateOccurred)
    Min.            1st Qu.       Median         Mean      3rd Qu.         Max.         NA's 
    "1400-06-30" "1999-09-06" "2004-01-10" "2001-02-13" "2007-07-26" "2010-08-30"          "1" 

Surprisingly, this data goes back quite a long time; the oldest UFO sighting comes from 1400! Given this outlier, the next question is: how is this data distributed over time? And is it worth analyzing the entire time series? A quick way to look at this visually is to construct a histogram. We will discuss histograms in more detail in the next chapter, but for now you should know that histograms allow you to bin your data by a given dimension and observe the frequency with which your data falls into those bins. The dimension of interest here is time, so we construct a histogram that bins the data over time:

    quick.hist<-ggplot(ufo.us, aes(x=DateOccurred))+geom_histogram()+ scale_x_date(major="50 years")
    ggsave(plot=quick.hist, filename="../images/quick_hist.png", height=6, width=8) stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this.

There are several things to note here. This is our first use of the ggplot2 package, which we use throughout this book for all of our data visualizations. In this case, we are constructing a very simple histogram that requires only a single line of code. First, we create a ggplot object and pass it the UFO data frame as its initial argument. Next, we set the x-axis aesthetic to the DateOccurred column, as this is the frequency we are interested in examining. With ggplot2 we must always work with data frames, and the first argument to create a ggplot object must always be a data frame. ggplot2 is an R implementation of Leland Wilkinson’s Grammar of Graphics [Wil05]. This means the package adheres to this particular philosophy for data visualization, and all visualiza- tions will be built up as a series of layers. For this histogram, shown in Figure 1-5, the initial layer is the x-axis data, namely the UFO sighting dates. Next, we add a histogram layer with the geom_histogram function. In this case, we will use the default settings for this function, but as we will see later, this default often is not a good choice. Finally, because this data spans such a long time period, we will rescale the x-axis labels to occur every 50 years with the scale_x_date function.
Once the ggplot object has been constructed, we use the ggsave function to output the visualization to a file. We also could have used > print(quick.hist) to print the visu- alization to the screen. Note the warning message that is printed when you draw the visualization. There are many ways to bin data in a histogram, and we will discuss this in detail in the next chapter, but this warning is provided to let you know exactly how ggplot2 does the binning by default.

We are now ready to explore the data with this visualization.

![Figure 1-5](images/figure 1-5.png?raw=true)

The results of this analysis are stark. The vast majority of the data occur between 1960 and 2010, with the majority of UFO sightings occurring within the last two decades. For our purposes, therefore, we will focus on only those sightings that occurred between 1990 and 2010. This will allow us to exclude the outliers and compare relatively similar units during the analysis. As before, we will use the subset function to create a new data frame that meets this criteria:

    ufo.us<-subset(ufo.us, DateOccurred>=as.Date("1990-01-01")) nrow(ufo.us)
    #[1] 46347

Although this removes many more entries than we eliminated while cleaning the data, it still leaves us with over 46,000 observations to analyze. To see the difference, we regenerate the histogram of the subset data in Figure 1-6. We see that there is much more variation when looking at this sample. Next, we must begin organizing the data such that it can be used to address our central question: what, if any, seasonal variation exists for UFO sightings in US states? To address this, we must first ask: what do we mean by “seasonal?” There are many ways to aggregate time series data with respect to seasons: by week, month, quarter, year, etc. But which way of aggregating our data is most appropriate here? The DateOccurred column provides UFO sighting information by the day, but there is considerable inconsistency in terms of the coverage throughout the entire set. We need to aggregate the data in a way that puts the amount of data for each state on relatively level planes. In this case, doing so by year-month is the best option. This aggregation also best addresses the core of our question, as monthly ag- gregation will give good insight into seasonal variations.

![Figure 1-6]

We need to count the number of UFO sightings that occurred in each state by all year- month combinations from 1990–2010. First, we will need to create a new column in the data that corresponds to the years and months present in the data. We will use the strftime function to convert the Date objects to a string of the “YYYY-MM” format. As before, we will set the format parameter accordingly to get the strings:

    ufo.us$YearMonth<-strftime(ufo.us$DateOccurred, format="%Y-%m")

Notice that in this case we did not use the transform function to add a new column to
the data frame. Rather, we simply referenced a column name that did not exist, and R automatically added it. Both methods for adding new columns to a data frame are useful, and we will switch between them depending on the particular task. Next, we want to count the number of times each state and year-month combination occurs in the data. For the first time we will use the ddply function, which is part of the extremely useful plyr library for manipulating data.

The plyr family of functions work a bit like the map-reduce-style data aggregation tools that have risen in popularity over the past several years. They attempt to group data in some specific way that was meaningful to all observations, and then do some calcula- tion on each of these groups and return the results. For this task we want to group the data by state abbreviations and the year-month column we just created. Once the data is grouped as such, we count the number of entries in each group and return that as a new column. Here we will simply use the nrow function to reduce the data by the number of rows in each group:

    sightings.counts<-ddply(ufo.us,.(USState,YearMonth), nrow) head(sightings.counts)
    USState YearMonth V1

    1 ak 2 ak 3 ak 4 ak 5 ak 6 ak
    1990-01 1 1990-03 1 1990-05 1 1993-11 1 1994-11 1 1995-01 1

We now have the number of UFO sightings for each state by the year and month. From the head call in the example, however, we can see that there may be a problem with using the data as is because it contains a lot of missing values. For example, we see that there was one UFO sighting in January, March, and May of 1990 in Alaska, but no entries appear for February and April. Presumably, there were no UFO sightings in these months, but the data does not include entries for nonsightings, so we have to go back and add these as zeros.
We need a vector of years and months that spans the entire data set. From this we can check to see whether they are already in the data, and if not, add them as zeros. To do this, we will create a sequence of dates using the seq.Date function, and then format them to match the data in our data frame:
    
    date.range<-seq.Date(from=as.Date(min(ufo.us$DateOccurred)), to=as.Date(max(ufo.us$DateOccurred)), by="month")
    date.strings<-strftime(date.range, "%Y-%m")

With the new date.strings vector, we need to create a new data frame that has all year- months and states. We will use this to perform the matching with the UFO sighting data. As before, we will use the lapply function to create the columns and the do.call function to convert this to a matrix and then a data frame:

    states.dates<-lapply(us.states,function(s) cbind(s,date.strings)) states.dates<-data.frame(do.call(rbind, states.dates), stringsAsFactors=FALSE) head(states.dates)
s date.strings

The states.dates data frame now contains entries for every year, month, and state combination possible in the data. Note that there are now entries from February and March 1990 for Alaska. To add in the missing zeros to the UFO sighting data, we need to merge this data with our original data frame. To do this, we will use the merge func- tion, which takes two ordered data frames and attempts to merge them by common columns. In our case, we have two data frames ordered alphabetically by US state abbreviations and chronologically by year and month. We need to tell the function which columns to merge these data frames by. We will set the by.x and by.y parameters according to the matching column names in each data frame. Finally, we set the all parameter to TRUE, which instructs the function to include entries that do not match and to fill them with NA. Those entries in the V1 column will be those state, year, and month entries for which no UFOs were sighted:

    all.sightings<-merge(states.dates,sightings.counts,by.x=c("s","date.strings"), by.y=c("USState","YearMonth"),all=TRUE)
    head(all.sightings)

The final steps for data aggregation are simple housekeeping. First, we will set the column names in the new all.sightings data frame to something meaningful. This is done in exactly the same way as we did it at the outset. Next, we will convert the NA entries to zeros, again using the is.na function. Finally, we will convert the YearMonth and State columns to the appropriate types. Using the date.range vector we created in the previous step and the rep function to create a new vector that repeats a given vector, we replace the year and month strings with the appropriate Date object. Again, it is better to keep dates as Date objects rather than strings because we can compare Date objects mathematically, but we can’t do that easily with strings. Likewise, the state abbreviations are better represented as categorical variables than strings, so we convert these to factor types. We will describe factors and other R data types in more detail in the next chapter:
    
    names(all.sightings)<-c("State","YearMonth","Sightings") all.sightings$Sightings[is.na(all.sightings$Sightings)]<-0 all.sightings$YearMonth<-as.Date(rep(date.range,length(us.states))) all.sightings$State<-as.factor(toupper(all.sightings$State))

We are now ready to analyze the data visually!



### Analyzing the data ###

For this data, we will address the core question only by analyzing it visually. For the remainder of the book, we will combine both numeric and visual analyses, but as this example is only meant to introduce core R programming paradigms, we will stop at the visual component. Unlike the previous histogram visualization, however, we will take greater care with ggplot2 to build the visual layers explicitly. This will allow us to create a visualization that directly addresses the question of seasonal variation among states over time and produce a more professional-looking visualization.
We will construct the visualization all at once in the following example, then explain each layer individually:

    state.plot<-ggplot(all.sightings, aes(x=YearMonth,y=Sightings))+ geom_line(aes(color="darkblue"))+ facet_wrap(~State,nrow=10,ncol=5)+
    theme_bw()+ scale_color_manual(values=c("darkblue"="darkblue"),legend=FALSE)+ scale_x_date(major="5 years", format="%Y")+ xlab("Time")+ylab("Number of Sightings")+
    opts(title="Number of UFO sightings by Month-Year and U.S. State (1990-2010)") ggsave(plot=state.plot, filename="../images/ufo_sightings.pdf",width=14,height=8.5)

As always, the first step is to create a ggplot object with a data frame as its first argument. Here we are using the all.sightings data frame we created in the previous step. Again, we need to build an aesthetic layer of data to plot, and in this case the x-axis is the YearMonth column and the y-axis is the Sightings data. Next, to show seasonal variation among states, we will plot a line for each state. This will allow us to observe any spikes, lulls, or oscillation in the number of UFO sightings for each state over time. To do this, we will use the geom_line function and set the color to "darkblue" to make the visual- ization easier to read.
As we have seen throughout this case, the UFO data is fairly rich and includes many sightings across the United States over a long period of time. Knowing this, we need to think of a way to break up this visualization such that we can observe the data for each state, but also compare it to the other states. If we plot all of the data in a single panel, it will be very difficult to discern variation. To check this, run the first line of code from the preceding block, but replace color="darkblue" with color=State and enter > print(state.plot) at the console. A better approach would be to plot the data for each state individually and order them in a grid for easy comparison.
To create a multifaceted plot, we use the facet_wrap function and specify that the panels be created by the State variable, which is already a factor type, i.e., categorical. We also explicitly define the number of rows and columns in the grid, which is easier in our case because we know we are creating 50 different plots.
The ggplot2 package has many plotting themes. The default theme is the one we used in the first example and has a gray background with dark gray gridlines. Although it is strictly a matter of taste, we prefer using a white background for this plot because that will make it easier to see slight differences among data points in our visualization. We add the theme_bw layer, which will produce a plot with a white background and black gridlines. Once you become more comfortable with ggplot2, we recommend experi- menting with different defaults to find the one you prefer.3
The remaining layers are done as housekeeping to make sure the visualization has a professional look and feel. Though not formally required, paying attention to these details is what can separate amateurish plots from professional-looking data visualiza- tions. The scale_color_manual function is used to specify that the string “darkblue” corresponds to the web-safe color “darkblue.” Although this may seem repetitive, it is at the core of ggplot2’s design, which requires explicit definition of details such as color. In fact, ggplot2 tends to think of colors as a way of distinguishing among different types or categories of data and, as such, prefers to have a factor type used to specify color. In our case we are defining a color explicitly using a string and therefore have to define the value of that string with the scale_color_manual function.
As we did before, we use the scale_x_date to specify the major gridlines in the visual- ization. Because this data spans 20 years, we will set these to be at regular five-year intervals. Then we set the tick labels to be the year in a full four-digit format. Next, we set the x-axis label to “Time” and the y-axis label to “Number of Sightings” by using the xlab and ylab functions, respectively. Finally, we use the opts function to give the plot a title.There are many more options available in the opts function, and we will see some of them in later chapters, but there are many more that are beyond the scope of this book.
With all of the layers built, we are now ready to render the image with ggsave and analyze the data.
There are many interesting observations that fall out of this analysis (see Figure 1-7). We see that California and Washington are large outliers in terms of the number of UFO sightings in these states compared to the others. Between these outliers, there are also interesting differences. In California, the number of reported UFO sightings seems to be somewhat random over time, but steadily increasing since 1995, whereas in Washington, the seasonal variation seems to be very consistent over time, with regular peaks and valleys in UFO sightings starting from about 1995.
We can also notice that many states experience sudden spikes in the number of UFO sightings reported. For example, Arizona, Florida, Illinois, and Montana seem to have experienced spikes around mid-1997, and Michigan, Ohio, and Oregon experienced similar spikes in late-1999. Only Michigan and Ohio are geographically close among these groups. If we do not believe that these are actually the result of extraterrestrial visitors, what are some alternative explanations? Perhaps there was increased vigilance among citizens to look to the sky as the millennium came to a close, causing heavier reporting of false sightings.
If, however, you are sympathetic to the notion that we may be regularly hosting visitors from outer space, there is also evidence to pique your curiosity. In fact, there is sur- prising regularity of these sightings in many states in the US, with evidence of regional clustering as well. It is almost as if the sightings really contain a meaningful pattern.

![Figure 1-7]


## 链接 ##
* [目录](<list.md>)
* [上一节 加载和安装R包](1.1.3.md)
* [下一节 R延伸阅读](1.1.5.md)