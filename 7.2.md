## 7.2 岭回归（Ridge Regression） ##
Now that we know a little bit about how to use optim, we can start to use optimization algorithms to implement our own version of ridge regression. Ridge regression is a specific kind of regression that incorporates regularization, which we discussed in Chapter 6. The only difference between ordinary least squares regression and ridge regression is the error function: ridge regression considers the size of the regression coefficients to be part of the error term, which encourages the coefficients to be small. In this example, this pushes the slope and intercept toward zero.
Other than changing our error function, the only added complexity to running ridge regression is that we now have to include an extra parameter, lambda, that adjudicates between the importance of minimizing the squared error of our predictions and mini- mizing the values of a and b so that we don’t overfit our data. The extra parameter for this regularized algorithm is called a hyperparameter and was discussed in some detail in Chapter 6. Once we’ve selected a value of lambda, we can write the ridge error func- tion as follows:

    ridge.error <- function(heights.weights, a, b, lambda) {
        predictions <- with(heights.weights, height.to.weight(Height, a, b))
        errors <- with(heights.weights, Weight - predictions) return(sum(errors ^ 2) + lambda * (a ^ 2 + b ^ 2))
    }

