7 优化: 密码破译
=====================

## 7.1 优化问题简介 ##
到目前为止，我们把在书中遇到的算法都当做黑盒来考虑，这样我们就可以专注于理解程序的输入和获得的结果。从本质上来说，我们是把这些机器学习算法当做我们进行预测任务的一些函数库而已。

在这一章，我们会研究一些用来实现基本机器学习算法的技术。最开始，我们用一个函数拟合最简单的单变量线性回归模型。这个例子可以激发我们观察如何把模型作为优化问题拟合到数据中。优化问题是指，我们有一个可以控制其设置的机器，并且有一种方式来度量当前的设置是不是好的。我们的目标是获得某个度量指标的最大值，即寻找到最优的设置方案。这个最优方案我们成为最优解，寻找最优解的过程叫做优化。

对优化是什么有了一个简单的了解之后，我们的主要工作就成了：建立一个简单的密码破译系统，这个破译系统把密码的解密作为一个优化问题。

Because we’re going to build our own linear regression function, let’s go back to our standard example data set: people’s heights and weights. As we’ve already done, we’re going to assume that we can predict weights by computing a function of their heights. Specifically, we’ll assume that a linear function works, which looks like this in R:
既然我们即将创建我们自己的线性回归函数，我们先回过头来看一下我们之前的例子：人群的身高和体重信息。正如我们之前做过的，我们认为自己可以根据一个人的身高来预测他的体重。也就是说，我们认为线性回归看起来是这么工作的：

    height.to.weight <- function(height, a, b)
    {
    return(a + b * height)
    }

In Chapter 5 we went through the details of computing the slope and intercept of this line using the lm function. In this example, the a parameter is the slope of the line and the b parameter is the intercept, which tells us how much a person whose height is zero should weigh.
在第五章，我们用lm函数来计算获得这条线的斜率和截距，即a是斜率，b是截距（指一个人的身高是0的情况下，他应该有多重）。

With this function in hand, how can we decide which values of a and b are best? This is where optimization comes in: we want to first define a measure of how well our function predicts weights from heights and then change the values of a and b until we predict as well as we possibly can.
根据这个函数，我们如何确定哪一个a和b是最优的？这就是优化问题所关心的：我们首先要定义一个度量方式来确定我们预测结果的好坏，然后不断的调整a和b的值尽量接近最优。

How do we do this? Well, lm already does all of this for us. It has a simple error function that it tries to optimize, and it finds the best values for a and b using a very specific algorithm that works only for ordinary linear regression.
那么我们如何做呢？lm函数已经为我们做了所有事情。它有一个简单的误差函数可以找到a、b的最优值，这个函数用了一个只对经典线性回归有用的特殊算法。

我们不妨执行lm来看看他们预测的a和b：

    heights.weights <- read.csv('data/01_heights_weights_genders.csv')
    coef(lm(Weight ~ Height, data = heights.weights)) 
    #(Intercept) Height
    #-350.737192 7.717288

Why are these reasonable choices for a and b? To answer that, we need to know what error function lm is using. As we said briefly in Chapter 5, lm is based on an error measure called “squared error,” which works in the following way:

1. Fix values for a and b.
2. Given a value for height, make a guess at the associated weight.
3. Take the true weight, and subtract the predicted weight. Call this the error.
4. Square the error.
5. Sumthesquarederrorsoverallofyourexamplestogetyoursumofsquarederrors.

>>>For interpretation, we usually take the mean rather than the sum, and compute square roots. But for optimization, none of that is truly nec- essary, so we can save some small amount of computational time by just calculating the sum of squared errors.



