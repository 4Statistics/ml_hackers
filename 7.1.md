第七章 优化: 密码破译
=====================

## 7.1 优化问题简介 ##
到目前为止，我们把在书中遇到的算法都当做黑盒来考虑，这样我们就可以专注于理解程序的输入和获得的结果。从本质上来说，我们是把这些机器学习算法当做我们进行预测任务的一些函数库而已。

在这一章，我们会研究一些用来实现基本机器学习算法的技术。最开始，我们用一个函数拟合最简单的单变量线性回归模型。这个例子可以激发我们观察如何把模型作为优化问题拟合到数据中。优化问题是指，我们有一个可以控制其设置的机器，并且有一种方式来度量当前的设置是不是好的。我们的目标是获得某个度量指标的最大值，即寻找到最优的设置方案。这个最优方案我们成为最优解，寻找最优解的过程叫做优化。

对优化是什么有了一个简单的了解之后，我们的主要工作就成了：建立一个简单的密码破译系统，这个破译系统把密码的解密作为一个优化问题。

Because we’re going to build our own linear regression function, let’s go back to our standard example data set: people’s heights and weights. As we’ve already done, we’re going to assume that we can predict weights by computing a function of their heights. Specifically, we’ll assume that a linear function works, which looks like this in R:
既然我们即将创建我们自己的线性回归函数，我们先回过头来看一下我们之前的例子：人群的身高和体重信息。正如我们之前做过的，我们认为自己可以根据一个人的身高来预测他的体重。也就是说，我们认为线性回归看起来是这么工作的：

    height.to.weight <- function(height, a, b) {
        return(a + b * height)
    }

在第五章，我们用lm函数来计算获得这条线的斜率和截距，即a是斜率，b是截距（假如一个人的身高是0的情况下，他应该有多重）。

With this function in hand, how can we decide which values of a and b are best? This is where optimization comes in: we want to first define a measure of how well our function predicts weights from heights and then change the values of a and b until we predict as well as we possibly can.
根据这个函数，我们如何确定哪一个a和b是最优的？这就是优化问题所关心的：我们首先要定义一个度量方式来确定我们预测结果的好坏，然后不断的调整a和b的值尽量接近最优。

How do we do this? Well, lm already does all of this for us. It has a simple error function that it tries to optimize, and it finds the best values for a and b using a very specific algorithm that works only for ordinary linear regression.
那么我们如何做呢？lm函数已经为我们做了所有事情。它有一个简单的误差函数可以找到a、b的最优值，这个函数用了一个只对经典线性回归有用的特殊算法。

我们不妨执行lm来看看他们预测的a和b：

    heights.weights <- read.csv('data/01_heights_weights_genders.csv')
    coef(lm(Weight ~ Height, data = heights.weights)) 
    #(Intercept) Height
    #-350.737192 7.717288

为什么选择这两个值作为a和b呢？要回答这个问题，我们需要知道lm用了什么误差函数。正如我们在第五章介绍的，lm是基于一种叫做“平方误差（squared error）”，它的工作过程如下：

1. 确定一组a、b值。
2. 给出一个height值，根据a、b预测相关的weight。
3. 取出该height对应的真实weight，减去预测的weight值，结果称为误差。
4. 对误差值做平方。
5. 针对所有的样本数据求差平方并相加，获得最终的平方误差的和。

>>>需要解释的是，我们常用平均值而不是求和，然后计算平方根。但对于优化来说，这种方法不是必须的，因此为了节省时间我们可以只计算所有误差平方的和。

The last two steps are closely related; if we weren’t summing the errors for each data point together, squaring wouldn’t be helpful. Squaring is essential precisely because summing all of the raw errors together would give us zero total error.

>>>要证明这一点总是对的并不难，但这需要一些我们在本书中一直想要避免的数学知识。

我们来看一下实现了这一过程的代码：

    squared.error <- function(heights.weights, a, b) {
        predictions <- with(heights.weights, height.to.weight(Height, a, b))
        errors <- with(heights.weights, Weight - predictions) 
        return(sum(errors ^ 2))
    }

我们通过给squared.error一些a、b值来感受一下它是如何运行的（结果显示在 Table 7-1）:

    for (a in seq(-1, 1, by = 1)) {
        for (b in seq(-1, 1, by = 1)) {
            print(squared.error(heights.weights, a, b)) 
        }
    }

![Table 7-1](images/table 7-1.png?raw=true "Table 7-1")

As you can see, some values of a and b give much lower values for squared.error than others. That means we really want to find the best values for a and b now that we have a meaningful error function that tells us something about our ability to make predic- tions. That’s the first part of our optimization problem: set up a metric that we can then try to minimize or maximize. That metric is usually called our objective function. The problem of optimization then becomes the problem of finding the best values for a and b to make that objective function as small or as big as possible.

一个明显的方法叫做网格搜索：先求出一个我们前面看到的，a、b取值范围足够大的表格，然后遍历这个表格依次取a、b值，不断的获得squared.error较小的结果。这种方法总是能够给出表格中的最优解，但是它有一些很严重的问题：

• 我们应该如何定义网格中每一组数据间的间隔？0，1，2，3？还是0，0.001，0.002，0.003？换句话说，我们应该如何进行我们的搜索。要回答这个问题，需要你对两种网格都进行评测，看看哪一个更有价值。这种选择两个网格哪种更好的过程又变成了一个优化问题，整个过程陷入了无限循环。

• 如果你想对具有两个参数的网格进行每个参数10个数据点的评测，你需要创建一个100行的表格。但如果参数达到了100个，那你就需要创建一个10^100行的表格。这种问题在机器学习领域非常常见，叫做维数灾难（Curse of Dimensionality）

由于我们希望能够进行数百上千维度的输入，因此网格搜索不是一个合适的算法。我们该怎么办？庆幸的是，计算机科学家和数学家思考这种优化问题已经很久了，他们建立了一大堆现成的你可以直接使用的优化算法。在R中，你应该首选optim函数来进行优化问题，它包含了很多当前最流行的优化算法。

为了展示optim是如何工作的，我们来用它拟合我们的线性回归模型。我们希望它的结果与跟用lm相比差不多：

    optim(c(0, 0), function (x){
        squared.error(heights.weights, x[1], x[2])
    })
    #$par
    #[1] -350.786736    7.718158
    #
    #$value
    #[1] 1492936
    #
    #$counts
    #function gradient 
    # 111       NA 
    #
    #$convergence
    #[1] 0
    #
    #$message
    #NULL

如上面的代码，optim接收接收了几个不同的参数。首先，你需要传递一个数值向量来表示你想要优化的参数初始值，在本例中，我们认为a和b应该从c(0,0)开始。然后，你应该传一个函数给optim来接收这个向量，我们把向量称为x。最后，我们把误差计算函数包装在匿名函数内。

Running this call to optim gives us values for a and b, respectively, as the values of par. And these values are very close to those produced by lm, which suggests that optim is working.1 In practice, lm uses an algorithm that’s much more specific to linear regression than our call to optim, which makes the results more precise than those produced by optim. If you’re going to work through your own problems, though, optim is much better to work with because it can be used with models other than linear regression.

The other outputs we get from optim are sometimes less interesting. The first we’ve shown is value, which tells us the value of the squared error evaluated at the parameters that optim claims are best. After that, the counts value tells us how many times optim evaluated the main function (here called function) and the gradient (here called gradient), which is an optional argument that can be passed if you know enough cal- culus to compute the gradient of the main function.

>>>如果“梯度”这个词你完全不懂，没有关系，我们也不喜欢手工的计算，因此我们通常不给optim指定任何参数来进行默认的梯度计算。虽然你的进度可能有所不同，但目前为止我们进行的还不错。

下面一个值叫做“收敛（convergence）”，用来告诉我们optim是找到了一组它认为是最优的结果。如果优化进行的正常的话，你应该会看到一个0值。如果你获得了一个非0值，你可以查看optim文档获得说明。最后，是其他有必要让用户看到的消息（message）。

In general, optim is based on a lot of clever ideas from calculus that help us perform optimization. Because it’s quite mathematically complex, we won’t get into the inner workings of optim at all. But the general spirit of what optim is doing is very simple to express graphically. Imagine that you only wanted to find the best value of a after you decided that b had to be 0. You could calculate the squared error when you vary only a using the following code:

    a.error <- function(a) {
        return(squared.error(heights.weights, a, 0)) 
    }

To get a sense of where the best value of a is, you can graph the squared error as a function of a using the curve function in R, which will evaluate a function or expression at a set of many values of a variable x and then plot the output of that function or the value of the expression. In the following example, we’ve evaluated a.error at many values of x, and because of a quirk in R’s evaluation of expressions, we’ve had to use sapply to get things to work.

    curve(sapply(x, function (a) {a.error(a)}), from = -1000, to = 1000)

![Figure 7-1](images/figure 7-1.png?raw=true "Figure 7-1")

Looking at Figure 7-1, there seems to be a single value for a that’s best, and every value that moves further away from that value for a is worse. When this happens, we say that there’s a global optimum. In cases like that, optim can use the shape of the squared error function to figure out in which direction to head after evaluating the error function at a single value of a; using this local information to learn something about the global structure of your problem lets optim hit the optimum very quickly.
To get a better sense of the full regression problem, we also need to look at how the error function responds when we change b:

    b.error <- function(b) {
        return(squared.error(heights.weights, 0, b))
    }
    curve(sapply(x, function (b) {b.error(b)}), from = -1000, to = 1000)

Looking at Figure 7-2, the error function looks like it also has a global optimum for b. Taken together with evidence that there’s a global optimum for a, this suggests that it should be possible for optim to find a single best value for both a and b that minimizes our error function.

![Figure 7-2](images/figure 7-2.png?raw=true "Figure 7-2")

More generally, we can say that optim works because it can search for the troughs of these graphs in all of the parameters at once by using calculus. It works faster than grid search because it can use information about the point it’s currently considering to infer something about nearby points. That lets it decide which direction it should move in to improve its performance. This adaptive behavior makes it much more efficient than grid search.

## 链接 ##
* [目录](<list.md>)
* [上一节 文本回归](6.3.md)
* [下一节 岭回归(Ridge Regression)](7.2.md)